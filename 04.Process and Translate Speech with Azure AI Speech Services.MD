# [Process and Translate Speech with Azure AI Speech Services](https://learn.microsoft.com/en-us/training/paths/process-translate-speech-azure-cognitive-apeech-services/)

## [Create speech-enabled apps with Azure AI services](https://learn.microsoft.com/en-us/training/modules/transcribe-speech-input-text/)
  * The Azure AI Speech service enables you to build speech-enabled applications. 
  * This module focuses on using the speech-to-text and text to speech APIs, which enable you to create apps that are capable of speech recognition and speech synthesis.
    1. Speech to text: An API that enables speech recognition in which your application can accept spoken input.
    1. Text to speech: An API that enables speech synthesis in which your application can provide spoken output.
    1. Speech Translation: An API that you can use to translate spoken input into multiple languages.
    1. Speaker Recognition: An API that enables your application to recognize individual speakers based on their voice.
    1. Intent Recognition: An API that uses conversational language understanding to determine the semantic meaning of spoken input.
  * Provision an Azure resource for speech - Use Azure AI Speech, use either a dedicated Azure AI Speech resource or a multi-service Azure AI Services resource.
  * In practice, most interactive speech-enabled applications use the Azure AI Speech service through a (programming) language-specific SDK.
  
### Use the Azure AI Speech to text API
  * The Azure AI Speech service supports speech recognition through two REST APIs:
    1. The Speech to text API, which is the primary way to perform speech recognition.
    1. The Speech to text Short Audio API, which is optimized for short streams of audio (up to 60 seconds).
  * Use either API for interactive speech recognition, depending on the expected length of the spoken input. 
  * Also use the Speech to text API for batch transcription, transcribing multiple audio files to text as a batch operation.
  * [Learn more about the REST APIs in the Speech to text REST API documentation.](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-speech-to-text) 
  * Using the Azure AI Speech SDK
    1. ![](https://learn.microsoft.com/en-us/training/wwl-data-ai/transcribe-speech-input-text/media/speech-to-text.png)
    1. While the specific details vary, depending on the SDK being used (Python, C#, and so on); there's a consistent pattern for using the Speech to text API:
       1. A SpeechRecognizer object is created from a SpeechConfig and AudioConfig, and its RecognizeOnceAsync method is used to call the Speech API
       1. Use a SpeechConfig object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key.
       1. Optionally, use an AudioConfig to define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file.
       1. Use the SpeechConfig and AudioConfig to create a SpeechRecognizer object. This object is a proxy client for the Speech to text API.
       1. Use the methods of the SpeechRecognizer object to call the underlying API functions. For example, the RecognizeOnceAsync() method uses the Azure AI Speech service to asynchronously transcribe a single spoken utterance.
       1. Process the response from the Azure AI Speech service. In the case of the RecognizeOnceAsync() method, the result is a SpeechRecognitionResult object that includes the following properties: Duration, OffsetInTicks, Properties, Reason, ResultId, Text
    1. If the operation was successful, the Reason property has the enumerated value RecognizedSpeech, and the Text property contains the transcription. 
    1. Other possible values for Result include NoMatch (indicating that the audio was successfully parsed but no speech was recognized) or Canceled, indicating that an error occurred (in which case, you can check the Properties collection for the CancellationReason property to determine what went wrong).

### Use the text to speech API
* Similarly to its Speech to text APIs, the Azure AI Speech service offers other REST APIs for speech synthesis:
	1. The Text to speech API, which is the primary way to perform speech synthesis.
	1. The Batch synthesis API, which is the batch operations that convert large volumes of text to audio - for example to generate an audio-book from the source text.
* [Learn learn more about the REST APIs in the Text to speech REST API documentation.](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-synthesis) 
* Using the Azure AI Speech SDK
	1. ![](https://learn.microsoft.com/en-us/training/wwl-data-ai/transcribe-speech-input-text/media/text-to-speech.png)
	1. The pattern for implementing speech synthesis is similar to that of speech recognition:
		1. A SpeechSynthesizer object is created from a SpeechConfig and AudioConfig, and its SpeakTextAsync method is used to call the Speech API
		1. Use a SpeechConfig object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key.
		1. Optionally, use an AudioConfig to define the output device for the speech to be synthesized. By default, this is the default system speaker, but you can also specify an audio file, or by explicitly setting this value to a null value, you can process the audio stream object that is returned directly.
		1. Use the SpeechConfig and AudioConfig to create a SpeechSynthesizer object. This object is a proxy client for the Text to speech API.
		1. Use the methods of the SpeechSynthesizer object to call the underlying API functions. For example, the SpeakTextAsync() method uses the Azure AI Speech service to convert text to spoken audio.
		1. Process the response from the Azure AI Speech service. In the case of the SpeakTextAsync method, the result is a SpeechSynthesisResult object that contains the following properties: AudioData, Properties, Reason, ResultId
	1. When speech has been successfully synthesized, the Reason property is set to the SynthesizingAudioCompleted enumeration and the AudioData property contains the audio stream (which, depending on the AudioConfig may have been automatically sent to a speaker or file).

### Configure audio format and voices
*	When synthesizing speech, you can use a SpeechConfig object to customize the audio that is returned by the Azure AI Speech service.
* Audio format
	1. The Azure AI Speech service supports multiple output formats for the audio stream that is generated by speech synthesis. 
	1. Depending on your specific needs, you can choose a format based on the required: Audio file type, Sample-rate, Bit-depth
	1. The supported formats are indicated in the SDK using the SpeechSynthesisOutputFormat enumeration. For example, SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm.
	1. To specify the required output format, use the SetSpeechSynthesisOutputFormat method of the SpeechConfig object:
		```
		C#
		speechConfig.SetSpeechSynthesisOutputFormat(SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm);
		```
* Voices
	1. The Azure AI Speech service provides multiple voices that you can use to personalize your speech-enabled applications. There are two kinds of voice that you can use:
		1. Standard voices - synthetic voices created from audio samples.
		1. Neural voices - more natural sounding voices created using deep neural networks.
	1. Voices are identified by names that indicate a locale and a person's name - for example en-GB-George.
	1. To specify a voice for speech synthesis in the SpeechConfig, set its SpeechSynthesisVoiceName property to the voice you want to use:
		```
		C#
		speechConfig.SpeechSynthesisVoiceName = "en-GB-George";
		```

### Use Speech Synthesis Markup Language
*	While the Azure AI Speech SDK enables you to submit plain text to be synthesized into speech (for example, by using the SpeakTextAsync() method), the service also supports an XML-based syntax for describing characteristics of the speech you want to generate. 
* This Speech Synthesis Markup Language (SSML) syntax offers greater control over how the spoken output sounds, enabling you to:
	1. Specify a speaking style, such as "excited" or "cheerful" when using a neural voice.
	1. Insert pauses or silence.
	1. Specify phonemes (phonetic pronunciations), for example to pronounce the text "SQL" as "sequel".
	1. Adjust the prosody of the voice (affecting the pitch, timbre, and speaking rate).
	1. Use common "say-as" rules, for example to specify that a given string should be expressed as a date, time, telephone number, or other form.
	1. Insert recorded speech or audio, for example to include a standard recorded message or simulate background noise.
* For example, consider the following SSML:
	```
	XML
	<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" 
										 xmlns:mstts="https://www.w3.org/2001/mstts" xml:lang="en-US"> 
		<voice name="en-US-AriaNeural"> 
				<mstts:express-as style="cheerful"> 
					I say tomato 
				</mstts:express-as> 
		</voice> 
		<voice name="en-US-GuyNeural"> 
				I say <phoneme alphabet="sapi" ph="t ao m ae t ow"> tomato </phoneme>. 
				<break strength="weak"/>Lets call the whole thing off! 
		</voice> 
	</speak>
	```
* This SSML specifies a spoken dialog between two different neural voices, like this:
	```
	Ariana (cheerfully): "I say tomato:
	Guy: "I say tomato (pronounced tom-ah-toe) ... Let's call the whole thing off!"
	```
* To submit an SSML description to the Speech service, you can use the SpeakSsmlAsync() method, like this:
	```
	C# 
	speechSynthesizer.SpeakSsmlAsync(ssml_string);
	```

### Excersie - Recognize and synthesize speech
*	[!NOTE] This exercise requires that you are using a computer with speakers/headphones. 
	1. For the best experience, a microphone is also required. 
	1. Some hosted virtual environments may be able to capture audio from your local microphone, but if this doesn’t work (or you don’t have a microphone at all), you can use a provided audio file for speech input. 
* Clone the repository for this course. Use 07-speech folder
* Provision an Azure AI Speech resource - Portal -> Azure AI services -> Speech service -> Create
* Prepare to use the Azure AI Speech service. Code
	1. install the Speech SDK package
	```
	C#
	dotnet add package Microsoft.CognitiveServices.Speech --version 1.30.0

	Python
	pip install azure-cognitiveservices-speech==1.30.0
	```
	1. Import namespaces.
	```
	C#
	 // Import namespaces
	 using Microsoft.CognitiveServices.Speech;
	 using Microsoft.CognitiveServices.Speech.Audio;
	
	Python
	 # Import namespaces
	 import azure.cognitiveservices.speech as speech_sdk
	```
	1. Main function
	```
	C#
	 // Configure speech service
	 speechConfig = SpeechConfig.FromSubscription(cogSvcKey, cogSvcRegion);
	 Console.WriteLine("Ready to use speech service in " + speechConfig.Region);
			
	 // Configure voice
	 speechConfig.SpeechSynthesisVoiceName = "en-US-AriaNeural";

	Python
	 # Configure speech service
	 speech_config = speech_sdk.SpeechConfig(cog_key, cog_region)
	 print('Ready to use speech service in:', speech_config.region)
	```
	
* Recognize speech
	1. a SpeechConfig for the speech service in your Azure AI Speech resource, use the Speech-to-text API to recognize speech and transcribe it to text.
	1. If you have a working microphone
		1. Main function, note that the code uses the TranscribeCommand function to accept spoken input.
		1. The code to create a SpeechRecognizer client that can be used to recognize and transcribe speech using the default system microphone:
			```
			C#
			 // Configure speech recognition
			 using AudioConfig audioConfig = AudioConfig.FromDefaultMicrophoneInput();
			 using SpeechRecognizer speechRecognizer = new SpeechRecognizer(speechConfig, audioConfig);
			 Console.WriteLine("Speak now...");
			
			Python
			 # Configure speech recognition
			 audio_config = speech_sdk.AudioConfig(use_default_microphone=True)
			 speech_recognizer = speech_sdk.SpeechRecognizer(speech_config, audio_config)
			 print('Speak now...')
			```
		1. Now skip ahead to the Add code to process the transcribed command section below.
	1. Alternatively, use audio input from a file
		1. Install a library that you can use to play the audio file:
			```
			C#
			 dotnet add package System.Windows.Extensions --version 4.6.0 
			
			Python
			 pip install playsound==1.3.0
			```
		1. Add namespace 
			```
			C#
			 using System.Media;
			
			Python
			 from playsound import playsound
			```
		1. Main function - the code to create a SpeechRecognizer client that can be used to recognize and transcribe speech from an audio file:
			```
			C#
			 // Configure speech recognition
			 string audioFile = "time.wav";
			 SoundPlayer wavPlayer = new SoundPlayer(audioFile);
			 wavPlayer.Play();
			 using AudioConfig audioConfig = AudioConfig.FromWavFileInput(audioFile);
			 using SpeechRecognizer speechRecognizer = new SpeechRecognizer(speechConfig, audioConfig);
			
			Python
			 # Configure speech recognition
			 current_dir = os.getcwd()
			 audioFile = current_dir + '\\time.wav'
			 playsound(audioFile)
			 audio_config = speech_sdk.AudioConfig(filename=audioFile)
			 speech_recognizer = speech_sdk.SpeechRecognizer(speech_config, audio_config)
			```
	1. Add code to process the transcribed command
		1. the code to listen for spoken input, being careful not to replace the code at the end of the function that returns the command:
			```
			C#
			 // Process speech input
			 SpeechRecognitionResult speech = await speechRecognizer.RecognizeOnceAsync();
			 if (speech.Reason == ResultReason.RecognizedSpeech)
			 {
					 command = speech.Text;
					 Console.WriteLine(command);
			 }
			 else
			 {
					 Console.WriteLine(speech.Reason);
					 if (speech.Reason == ResultReason.Canceled)
					 {
							 var cancellation = CancellationDetails.FromResult(speech);
							 Console.WriteLine(cancellation.Reason);
							 Console.WriteLine(cancellation.ErrorDetails);
					 }
			 }
			
			Python
			 # Process speech input
			 speech = speech_recognizer.recognize_once_async().get()
			 if speech.reason == speech_sdk.ResultReason.RecognizedSpeech:
					 command = speech.text
					 print(command)
			 else:
					 print(speech.reason)
					 if speech.reason == speech_sdk.ResultReason.Canceled:
							 cancellation = speech.cancellation_details
							 print(cancellation.reason)
							 print(cancellation.error_details)
			```
	1. Save & Run, Say “what time is it?”. The program should transcribe your spoken input and display the time.
	1. The SpeechRecognizer gives you around 5 seconds to speak. If it detects no spoken input, it produces a “No match” result.
	1. If the SpeechRecognizer encounters an error, it produces a result of “Cancelled”. The code in the application will then display the error message. The most likely cause is an incorrect key or region in the configuration file.

* Synthesize speech
	1. Application accepts spoken input, but it doesn’t actually speak! Let’s fix that by adding code to synthesize speech.
	1. the TellTime function to tell the user the current time. The code to create a SpeechSynthesizer client that can be used to generate spoken output:
		```
		C#
		 // Configure speech synthesis
		 speechConfig.SpeechSynthesisVoiceName = "en-GB-RyanNeural";
		 using SpeechSynthesizer speechSynthesizer = new SpeechSynthesizer(speechConfig);
		
		Python
		 # Configure speech synthesis
		 speech_config.speech_synthesis_voice_name = "en-GB-RyanNeural"
		 speech_synthesizer = speech_sdk.SpeechSynthesizer(speech_config)
		```
	1. [!NOTE] The default audio configuration uses the default system audio device for output, so you don’t need to explicitly provide an AudioConfig. 
	1. If you need to redirect audio output to a file, you can use an AudioConfig with a filepath to do so.
		```
		C#
		 // Synthesize spoken output
		 SpeechSynthesisResult speak = await speechSynthesizer.SpeakTextAsync(responseText);
		 if (speak.Reason != ResultReason.SynthesizingAudioCompleted)
		 {
				 Console.WriteLine(speak.Reason);
		 }
		
		Python
		 # Synthesize spoken output
		 speak = speech_synthesizer.speak_text_async(response_text).get()
		 if speak.reason != speech_sdk.ResultReason.SynthesizingAudioCompleted:
				 print(speak.reason)
		```
	1. say “what time is it?”. The program should speak, telling you the time.

* Use a different voice
	```
	C#
	 // Configure speech synthesis
	 speechConfig.SpeechSynthesisVoiceName = "en-GB-LibbyNeural"; // change this
	 using SpeechSynthesizer speechSynthesizer = new SpeechSynthesizer(speechConfig);
	
	Python
	 # Configure speech synthesis
	 speech_config.speech_synthesis_voice_name = 'en-GB-LibbyNeural' # change this
	 speech_synthesizer = speech_sdk.SpeechSynthesizer(speech_config)
	```

* Use Speech Synthesis Markup Language
	1. Speech Synthesis Markup Language (SSML) enables you to customize the way your speech is synthesized using an XML-based format.
		```		
		C#
		 // Synthesize spoken output
		 string responseSsml = $@"
				 <speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='en-US'>
						 <voice name='en-GB-LibbyNeural'>
								 {responseText}
								 <break strength='weak'/>
								 Time to end this lab!
						 </voice>
				 </speak>";
		 SpeechSynthesisResult speak = await speechSynthesizer.SpeakSsmlAsync(responseSsml);
		 if (speak.Reason != ResultReason.SynthesizingAudioCompleted)
		 {
				 Console.WriteLine(speak.Reason);
		 }
		
		Python
		 # Synthesize spoken output
		 responseSsml = " \
				 <speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='en-US'> \
						 <voice name='en-GB-LibbyNeural'> \
								 {} \
								 <break strength='weak'/> \
								 Time to end this lab! \
						 </voice> \
				 </speak>".format(response_text)
		 speak = speech_synthesizer.speak_ssml_async(responseSsml).get()
		 if speak.reason != speech_sdk.ResultReason.SynthesizingAudioCompleted:
				 print(speak.reason)
		```
	1. Say “what time is it?”. The program should speak in the voice that is specified in the SSML (overriding the voice specified in the SpeechConfig), telling you the time, and then after a pause telling you it’s time to end this lab - which it is!

### Quiz
1. What information do you need from your Azure AI Speech service resource to consume it using the Azure AI Speech SDK? 
	1. [x] The location and one of the keys
	1. [ ] The primary and secondary keys
	1. [ ] The endpoint and one of the keys
1. Which object should you use to specify that the speech input to be transcribed to text is in an audio file? 
	1. [ ] SpeechConfig
	1. [x] AudioConfig
	1. [ ] SpeechRecognizer
1. How can you change the voice used in speech synthesis? 
	1. [ ] Specify a SpeechSynthesisOutputFormat enumeration in the SpeechConfig object.
	1. [x] Set the SpeechSynthesisVoiceName property of the SpeechConfig object to the desired voice name.
	1. [ ] Specify a filename in the AudioConfig object.

## [Translate speech with the Azure AI Speech service](https://learn.microsoft.com/en-us/training/modules/translate-speech-speech-service/)
	* Translation of speech builds on speech recognition by recognizing and transcribing spoken input in a specified language, and returning translations of the transcription in one or more other languages.
	* Provision an Azure resource for speech translation
		1. The Azure AI Speech service provides robust, ML and AI-based speech translation services, enabling developers to add end-to-end, real-time, speech translations to their applications or services. 
		1. Use either a dedicated Azure AI Speech resource or a multi-service Azure AI Services resource.

	* Translate speech to text
		1. ![](https://learn.microsoft.com/en-us/training/wwl-data-ai/translate-speech-speech-service/media/translate-speech-small.png)
		1. The pattern for speech translation using the Azure AI Speech SDK is similar to speech recognition, with the addition of information about the source and target languages for translation:
			1. A TranslationRecognizer object is created from a SpeechConfig, TranslationConfig, and AudioConfig; and its RecognizeOnceAsync method is used to call the Speech API.
			1. Use a SpeechTranslationConfig object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key.
			1. The SpeechTranslationConfig object is also used to specify the speech recognition language (the language in which the input speech is spoken) and the target languages into which it should be translated.
			1. Optionally, use an AudioConfig to define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file.
			1. Use the SpeechTranslationConfig, and AudioConfig to create a TranslationRecognizer object. This object is a proxy client for the Azure AI Speech translation API.
			1. Use the methods of the TranslationRecognizer object to call the underlying API functions. For example, the RecognizeOnceAsync() method uses the Azure AI Speech service to asynchronously translate a single spoken utterance.
			1. Process the response from Azure AI Speech. In the case of the RecognizeOnceAsync() method, the result is a SpeechRecognitionResult object that includes the following properties: Duration, OffsetInTicks, Properties, Reason, ResultId, Text, Translations
		1. If the operation was successful, the Reason property has the enumerated value RecognizedSpeech, the Text property contains the transcription in the original language. You can also access a Translations property which contains a dictionary of the translations (using the two-character ISO language code, such as "en" for English, as a key). 

	* Synthesize translations
		1. The TranslationRecognizer returns translated transcriptions of spoken input - essentially translating audible speech to text.
		1. You can also synthesize the translation as speech to create speech-to-speech translation solutions. There are two ways you can accomplish this.
		1. Event-based synthesis
			1. When you want to perform 1:1 translation (translating from one source language into a single target language), you can use event-based synthesis to capture the translation as an audio stream. To do this, you need to:
			1. Specify the desired voice for the translated speech in the TranslationConfig. 
			1. Create an event handler for the TranslationRecognizer object's Synthesizing event. 
			1. In the event handler, use the GetAudio() method of the Result parameter to retrieve the byte stream of translated audio. 
		
	* Manual synthesis
		1. Manual synthesis is an alternative approach to event-based synthesis that doesn't require you to implement an event handler. You can use manual synthesis to generate audio translations for one or more target languages.
		1. Manual synthesis of translations is essentially just the combination of two separate operations in which you:
			1. Use a TranslationRecognizer to translate spoken input into text transcriptions in one or more target languages.
			1. Iterate through the Translations dictionary in the result of the translation operation, using a SpeechSynthesizer to synthesize an audio stream for each language.

### Exercise - Translate Speech
	* develop a translator application that people can use when traveling in places where they don’t speak the local language. 
	* They would be able to say phrases such as “Where is the station?” or “I need to find a pharmacy” in their own language, and have it translate them to the local language.
	* Use Portal -> Azure AI Speech service -> Speech service
	* 08-speech-translation Code - Client application that uses the Azure AI Speech SDK to recognize, translate, and synthesize speech.
		```
		#install the Speech SDK package
		C#
		 dotnet add package Microsoft.CognitiveServices.Speech --version 1.30.0
		
		Python
		 pip install azure-cognitiveservices-speech==1.30.0
		
		C#
		 // Import namespaces
		 using Microsoft.CognitiveServices.Speech;
		 using Microsoft.CognitiveServices.Speech.Audio;
		 using Microsoft.CognitiveServices.Speech.Translation;
		
		 // Configure translation
		 translationConfig = SpeechTranslationConfig.FromSubscription(cogSvcKey, cogSvcRegion);
		 translationConfig.SpeechRecognitionLanguage = "en-US";
		 translationConfig.AddTargetLanguage("fr");
		 translationConfig.AddTargetLanguage("es");
		 translationConfig.AddTargetLanguage("hi");
		 Console.WriteLine("Ready to translate from " + translationConfig.SpeechRecognitionLanguage);
		
		 // Configure speech
		 speechConfig = SpeechConfig.FromSubscription(cogSvcKey, cogSvcRegion);
		
		Python
		 # Import namespaces
		 import azure.cognitiveservices.speech as speech_sdk
		
		 # Configure translation
		 translation_config = speech_sdk.translation.SpeechTranslationConfig(cog_key, cog_region)
		 translation_config.speech_recognition_language = 'en-US'
		 translation_config.add_target_language('fr')
		 translation_config.add_target_language('es')
		 translation_config.add_target_language('hi')
		 print('Ready to translate from',translation_config.speech_recognition_language)
		
		 # Configure speech
		 speech_config = speech_sdk.SpeechConfig(cog_key, cog_region)
		```
	* Implement speech translation
		```
		C#
		 // Translate speech
		 using AudioConfig audioConfig = AudioConfig.FromDefaultMicrophoneInput();
		 using TranslationRecognizer translator = new TranslationRecognizer(translationConfig, audioConfig);
		 Console.WriteLine("Speak now...");
		 TranslationRecognitionResult result = await translator.RecognizeOnceAsync();
		 Console.WriteLine($"Translating '{result.Text}'");
		 translation = result.Translations[targetLanguage];
		 Console.OutputEncoding = Encoding.UTF8;
		 Console.WriteLine(translation);
		
		Python
		 # Translate speech
		 audio_config = speech_sdk.AudioConfig(use_default_microphone=True)
		 translator = speech_sdk.translation.TranslationRecognizer(translation_config, audio_config = audio_config)
		 print("Speak now...")
		 result = translator.recognize_once_async().get()
		 print('Translating "{}"'.format(result.text))
		 translation = result.translations[targetLanguage]
		 print(translation)
		```
		1. NOTE The code in your application translates the input to all three languages in a single call. 
		1. Only the translation for the specific language is displayed, but you could retrieve any of the translations by specifying the target language code in the translations collection of the result.
	* Alternatively, use audio input from a file
		```
		C#
		 dotnet add package System.Windows.Extensions --version 4.6.0 
		 
			using System.Media;
		
		Python
		 pip install playsound==1.3.0
		
		 from playsound import playsound
		```
	* Translate function to translate spoken input, create a TranslationRecognizer client that can be used to recognize and translate speech from a file.
		```
		C#
		 // Translate speech
		 string audioFile = "station.wav";
		 SoundPlayer wavPlayer = new SoundPlayer(audioFile);
		 wavPlayer.Play();
		 using AudioConfig audioConfig = AudioConfig.FromWavFileInput(audioFile);
		 using TranslationRecognizer translator = new TranslationRecognizer(translationConfig, audioConfig);
		 Console.WriteLine("Getting speech from file...");
		 TranslationRecognitionResult result = await translator.RecognizeOnceAsync();
		 Console.WriteLine($"Translating '{result.Text}'");
		 translation = result.Translations[targetLanguage];
		 Console.OutputEncoding = Encoding.UTF8;
		 Console.WriteLine(translation);
		
		Python
		 # Translate speech
		 audioFile = 'station.wav'
		 playsound(audioFile)
		 audio_config = speech_sdk.AudioConfig(filename=audioFile)
		 translator = speech_sdk.translation.TranslationRecognizer(translation_config, audio_config = audio_config)
		 print("Getting speech from file...")
		 result = translator.recognize_once_async().get()
		 print('Translating "{}"'.format(result.text))
		 translation = result.translations[targetLanguage]
		 print(translation)
		```
		1. Enter a valid language code (fr, es, or hi), and then, say “where is the station?” or some other phrase you might use when traveling abroad. 
		1. The program should transcribe your spoken input and translate it to the language you specified (French, Spanish, or Hindi). 
		1. NOTE The TranslationRecognizer gives you around 5 seconds to speak. 
		1. If it detects no spoken input, it produces a “No match” result. 
		1. The translation to Hindi may not always be displayed correctly in the Console window due to character encoding issues.
	
	* Synthesize the translation to speech - the translation spoken aloud in a suitable voice.
		```
		C#
		 // Synthesize translation
		 var voices = new Dictionary<string, string>
		                 {
		                     ["fr"] = "fr-FR-HenriNeural",
		                     ["es"] = "es-ES-ElviraNeural",
		                     ["hi"] = "hi-IN-MadhurNeural"
		                 };
		 speechConfig.SpeechSynthesisVoiceName = voices[targetLanguage];
		 using SpeechSynthesizer speechSynthesizer = new SpeechSynthesizer(speechConfig);
		 SpeechSynthesisResult speak = await speechSynthesizer.SpeakTextAsync(translation);
		 if (speak.Reason != ResultReason.SynthesizingAudioCompleted)
		 {
		     Console.WriteLine(speak.Reason);
		 }
		
		Python
		 # Synthesize translation
		 voices = {
		         "fr": "fr-FR-HenriNeural",
		         "es": "es-ES-ElviraNeural",
		         "hi": "hi-IN-MadhurNeural"
		 }
		 speech_config.speech_synthesis_voice_name = voices.get(targetLanguage)
		 speech_synthesizer = speech_sdk.SpeechSynthesizer(speech_config)
		 speak = speech_synthesizer.speak_text_async(translation).get()
		 if speak.reason != speech_sdk.ResultReason.SynthesizingAudioCompleted:
		     print(speak.reason)
		```
	1. NOTE In this example, you’ve used a SpeechTranslationConfig to translate speech to text, and then used a SpeechConfig to synthesize the translation as speech.
	1. Use the SpeechTranslationConfig to synthesize the translation directly, but this only works when translating to a single language, and results in an audio stream that is typically saved as a file rather than sent directly to a speaker.

### Quiz
1. Which SDK object should you use to specify the language(s) into which you want speech translated? 
	1. [ ] SpeechConfig
	1. [x] SpeechTranslationConfig
	1. [ ] AudioConfig
2. Which SDK object should you use as a proxy for the Translation API of Azure AI Speech service? 
	1. [x] TranslationRecognizer
	1. [ ] SpeechRecognizer
	1. [ ] SpeechSynthesizer
3. When translating speech, in which cases can you use the Synthesizing event to synthesize the translations and speech? 
	1. [x] Only when translating to a single target language.
	1. [ ] Only when translating to multiple target languages.
	1. [ ] When translating to one or more target languages.
