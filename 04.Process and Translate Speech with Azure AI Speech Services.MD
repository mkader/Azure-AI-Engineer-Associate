# [Process and Translate Speech with Azure AI Speech Services](https://learn.microsoft.com/en-us/training/paths/process-translate-speech-azure-cognitive-apeech-services/)

## [Create speech-enabled apps with Azure AI services](https://learn.microsoft.com/en-us/training/modules/transcribe-speech-input-text/)
  * The Azure AI Speech service enables you to build speech-enabled applications. 
  * This module focuses on using the speech-to-text and text to speech APIs, which enable you to create apps that are capable of speech recognition and speech synthesis.
    1. Speech to text: An API that enables speech recognition in which your application can accept spoken input.
    1. Text to speech: An API that enables speech synthesis in which your application can provide spoken output.
    1. Speech Translation: An API that you can use to translate spoken input into multiple languages.
    1. Speaker Recognition: An API that enables your application to recognize individual speakers based on their voice.
    1. Intent Recognition: An API that uses conversational language understanding to determine the semantic meaning of spoken input.
  * Provision an Azure resource for speech - Use Azure AI Speech, use either a dedicated Azure AI Speech resource or a multi-service Azure AI Services resource.

### Use the Azure AI Speech to text API
  * The Azure AI Speech service supports speech recognition through two REST APIs:
    1. The Speech to text API, which is the primary way to perform speech recognition.
    1. The Speech to text Short Audio API, which is optimized for short streams of audio (up to 60 seconds).
  * Use either API for interactive speech recognition, depending on the expected length of the spoken input. 
  * Also use the Speech to text API for batch transcription, transcribing multiple audio files to text as a batch operation.
  * [Learn more about the REST APIs in the Speech to text REST API documentation.](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-speech-to-text) 
  * In practice, most interactive speech-enabled applications use the Speech service through a (programming) language-specific SDK.
  * Using the Azure AI Speech SDK
    1. ![](https://learn.microsoft.com/en-us/training/wwl-data-ai/transcribe-speech-input-text/media/speech-to-text.png)
    1. While the specific details vary, depending on the SDK being used (Python, C#, and so on); there's a consistent pattern for using the Speech to text API:
       1. A SpeechRecognizer object is created from a SpeechConfig and AudioConfig, and its RecognizeOnceAsync method is used to call the Speech API
       1. Use a SpeechConfig object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key.
       1. Optionally, use an AudioConfig to define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file.
       1. Use the SpeechConfig and AudioConfig to create a SpeechRecognizer object. This object is a proxy client for the Speech to text API.
       1. Use the methods of the SpeechRecognizer object to call the underlying API functions. For example, the RecognizeOnceAsync() method uses the Azure AI Speech service to asynchronously transcribe a single spoken utterance.
       1. Process the response from the Azure AI Speech service. In the case of the RecognizeOnceAsync() method, the result is a SpeechRecognitionResult object that includes the following properties: Duration, OffsetInTicks, Properties, Reason, ResultId, Text
    1. If the operation was successful, the Reason property has the enumerated value RecognizedSpeech, and the Text property contains the transcription. 
    1. Other possible values for Result include NoMatch (indicating that the audio was successfully parsed but no speech was recognized) or Canceled, indicating that an error occurred (in which case, you can check the Properties collection for the CancellationReason property to determine what went wrong).
