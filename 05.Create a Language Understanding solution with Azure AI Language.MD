# [Create a Language Understanding solution with Azure AI Language](https://learn.microsoft.com/en-us/training/paths/create-language-solution-azure-cognitive-services/)

## [Build a conversational language understanding model](https://learn.microsoft.com/en-us/training/modules/build-language-understanding-model/)

### Understand resources for building a conversational language understanding model
* To use the Language Understanding service to develop an NLP solution, create a Language resource in Azure. It will be used for both authoring your model and processing prediction requests from client applications.
* Azure AI Language has various features, including sentiment analysis, key phrase extraction, entity recognition, intent recognition, and text classification. 
* Some features, such as conversational language understanding and custom named entity recognition require a model to be built for prediction.
* Build your model
  1. For features that require a model for prediction - build, train and deploy that model before using it to make a prediction. This building and training will teach the Azure AI Language service what to look for.
  1. Portal -> Azure AI services -> Language Service -> Create -> Need a key and the endpoint
* Use the REST API - One way to build your model
  1. The pattern would be to create your project, import data, train, deploy, then use your model.
  1. These tasks are done asynchronously; submit a request to the appropriate URI for each step, and then send another request to get the status of that job.
  1. Example, if deploy a model for a conversational language understanding project, you'd submit the deployment job, and then check on the deployment job status.
* Authentication - providing the following header. Key	-> Ocp-Apim-Subscription-Key,  Value -> The key to your resource
* Request deployment
  1. Submit a POST request to the following endpoint.
    ```
    {ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}?api-version={API-VERSION}
    ```
  | Placeholder |	Value |	Example |
  |--|--|--|
  | {ENDPOINT} | The endpoint of your Azure AI Language resource |	https://<your-subdomain>.cognitiveservices.azure.com |
  | {PROJECT-NAME} | The name for your project. This value is case-sensitive |		myProject |
  | {DEPLOYMENT-NAME} | The name for your deployment. This value is case-sensitive |		staging |
  | {API-VERSION} | The version of the API you're calling |		2022-05-01 |
  
  1. Include the following body with your request. {MODEL-NAME} - 	The model name that will be assigned to your deployment. This value is case-sensitive.
    ```
    {
      "trainedModelLabel": "{MODEL-NAME}",
    }
    ```
  1. Successfully submitting your request will receive a 202 response, with a response header of operation-location. This header will have a URL with which to request the status, formatted like this:
    ```
    {ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}
    ```
  1. Get deployment status - Submit a GET request to the URL from the response header above. 
    ```
    {ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}
    ```
    1. {ENDPOINT}	- The endpoint for authenticating your API request
    1. {PROJECT-NAME}	- The name for your project (case-sensitive)
    1. {DEPLOYMENT-NAME} -	The name for your deployment (case-sensitive)
    1. {JOB-ID}	- The ID for locating your model's training status, found in the header value detailed above in the deployment request
    1. {API-VERSION} -	The version of the API you're calling
  1. The response body will give the deployment status details. The status field will have the value of succeeded when the deployment is complete.
    ```
    {
        "jobId":"{JOB-ID}",
        "createdDateTime":"String",
        "lastUpdatedDateTime":"String",
        "expirationDateTime":"String",
        "status":"running"
    }
    ```
  1. [walkthrough with example, the conversational understanding quickstart.](https://learn.microsoft.com/en-us/azure/ai-services/language-service/conversational-language-understanding/quickstart?pivots=rest-api#create-a-clu-project)

* Use Language Studio
  1. Visual method of building, training, and deploying your model
  1. Choose to create a Conversational language understanding project. 
  1. Once the project is created, then go through the same process as above to build, train, and deploy your model.
  1. ![](https://learn.microsoft.com/en-us/training/wwl-data-ai/build-language-understanding-model/media/language-studio-conversational-small.png)

* Query your model
  1. To query your model for a prediction, create a POST request to the appropriate URL with the appropriate body specified. 
  1. For built in features such as language detection or sentiment analysis, you'll query the analyze-text endpoint.
    ```
    {ENDPOINT}/language/:analyze-text?api-version={API-VERSION}
    
      1. {ENDPOINT}	The endpoint for authenticating your API request
      2. {API-VERSION}	The version of the API you're calling
    ```
  1. Within the body of that request, you must specify the kind parameter, which tells the service what type of language understanding you're requesting.
  1. If you want to detect the language, for example, the JSON body would look something like the following.
    ```
    {
        "kind": "LanguageDetection",
        "parameters": {
            "modelVersion": "latest"
        },
        "analysisInput":{
            "documents":[
                {
                    "id":"1",
                    "text": "This is a document written in English."
                }
            ]
        }
    }
    ```
  1. A sample response to your query would be similar to the following.
    ```
    {
        "kind": "LanguageDetectionResults",
        "results": {
            "documents": [{
                "id": "1",
                "detectedLanguage": {
                    "name": "English",
                    "iso6391Name": "en",
                    "confidenceScore": 1.0
                },
                "warnings": []
            }],
            "errors": [],
            "modelVersion": "{VERSION}"
        }
    }
    ```
  1. Other language features, such as the conversational language understanding discussed above, require the request be routed to a different endpoint. 
  1. For example, the conversational language understanding request would be sent to the following.
    ```
    {ENDPOINT}/language/:analyze-conversations?api-version={API-VERSION}
      1. {ENDPOINT}	The endpoint for authenticating your API request
      2. {API-VERSION}	The version of the API you're calling
    ```
  1. That request would include a JSON body similar to the following.
    ```
    {
      "kind": "Conversation",
      "analysisInput": {
        "conversationItem": {
          "id": "1",
          "participantId": "1",
          "text": "Sample text"
        }
      },
      "parameters": {
        "projectName": "{PROJECT-NAME}",
        "deploymentName": "{DEPLOYMENT-NAME}",
        "stringIndexType": "TextElement_V8"
      }
    }
    
      1. {PROJECT-NAME}	The name of the project where you built your model
      2. {DEPLOYMENT-NAME}	The name of your deployment
    ````
  1. A sample response to your query would be similar to the following.
    ```
    {
      "kind": "ConversationResult",
      "result": {
        "query": "String",
        "prediction": {
          "topIntent": "intent1",
          "projectKind": "Conversation",
          "intents": [
            {
              "category": "intent1",
              "confidenceScore": 1
            },
            {
              "category": "intent2",
              "confidenceScore": 0
            }
          ],
          "entities": [
            {
              "category": "entity1",
              "text": "text",
              "offset": 7,
              "length": 4,
              "confidenceScore": 1
            }
          ]
        }
      }
    }
    ```
  1. [documentation on features, examples and how-to guides, see the Azure AI Language pages.](https://learn.microsoft.com/en-us/azure/ai-services/language-service/)

### Define intents, utterances, and entities
* Utterances are the phrases that a user might enter when interacting with an application that uses your language model. 
* An intent represents a task or action the user wants to perform, or more simply the meaning of an utterance. 
* You create a model by defining intents and associating them with one or more utterances.
* For example, consider the following list of intents and associated utterances:
    ```
    GetTime:
      "What time is it?"
      "What is the time?"
      "Tell me the time"
    GetWeather:
      "What is the weather forecast?"
      "Do I need an umbrella?"
      "Will it snow?"
    TurnOnDevice
      "Turn the light on."
      "Switch on the light."
      "Turn on the fan"
    None:
      "Hello"
      "Goodbye"
    ```
* In your model, must define the intents that you want your model to understand, your model must support and the kinds of actions or information that users might request. 
* In addition to the intents that you define, every model includes a None intent that you should use to explicitly identify utterances that a user might submit, but for which there is no specific action required (for example, conversational greetings like "hello") or that fall outside of the scope of the domain for this model.
* After you've identified the intents your model must support, it's important to capture various different example utterances for each intent. Collect utterances that you think users will enter; including utterances meaning the same thing but that are constructed in different ways. Keep these guidelines in mind:
  1. Capture multiple different examples, or alternative ways of saying the same thing
  1. Vary the length of the utterances from short, to medium, to long
  1. Vary the location of the noun or subject of the utterance. Place it at the beginning, the end, or somewhere in between
  1. Use correct grammar and incorrect grammar in different utterances to offer good training data examples
  1. The precision, consistency and completeness of your labeled data are key factors to determining model performance.
      1. Label precisely: Label each entity to its right type always. Only include what you want extracted, avoid unnecessary data in your labels.
      1. Label consistently: The same entity should have the same label across all the utterances.
      1. Label completely: Label all the instances of the entity in all your utterances.
* Entities are used to add specific context to intents. 
  1. For example, you might define a TurnOnDevice intent that can be applied to multiple devices, and use entities to define the different devices.
  1. Consider the following utterances, intents, and entities:

  | Utterance	| Intent	| Entities |
  |--|--|--|
  | What is the time?	| GetTime |	 |
  | What time is it in London?	| GetTime	| Location (London) |
  | What's the weather forecast for Paris?	| GetWeather	| Location (Paris) |
  | Will I need an umbrella tonight?	| GetWeather	| Time (tonight) |
  | What's the forecast for Seattle tomorrow?	| GetWeather	| Location (Seattle), Time (tomorrow) |
  | Turn the light on.	| TurnOnDevice	| Device (light) |
  | Switch on the fan.	| TurnOnDevice	| Device (fan) |

* You can split entities into a few different component types:
  1. Learned entities are the most flexible kind of entity, and should be used in most cases. You define a learned component with a suitable name, and then associate words or phrases with it in training utterances. When you train your model, it learns to match the appropriate elements in the utterances with the entity.
  1. List entities are useful when you need an entity with a specific set of possible values - for example, days of the week. You can include synonyms in a list entity definition, so you could define a DayOfWeek entity that includes the values "Sunday", "Monday", "Tuesday", and so on; each with synonyms like "Sun", "Mon", "Tue", and so on.
  1. Prebuilt entities are useful for common types such as numbers, datetimes, and names. For example, when prebuilt components are added, you will automatically detect values such as "6" or organizations such as "Microsoft". You can see this article for a list of supported prebuilt entities.

### Use patterns to differentiate similar utterances
* In some cases, a model might contain multiple intents for which utterances are likely to be similar. 
* You can use the pattern of utterances to disambiguate the intents while minimizing the number of sample utterances.
* For example, consider the following utterances:
    1. "Turn on the kitchen light"
    1. "Is the kitchen light on?"
    1. "Turn off the kitchen light"
* These utterances are syntactically similar, with only a few differences in words or punctuation. 
* However, they represent three different intents (which could be named TurnOnDevice, GetDeviceStatus, and TurnOffDevice). 
* Additionally, the intents could apply to a wide range of entity values. 
* In addition to "kitchen light", the intent could apply to "living room light", television", or any other device that the model might need to support.
* To correctly train your model, provide a handful of examples of each intent that specify the different formats of utterances.
    ```
    TurnOnDevice:
      "Turn on the {DeviceName}"
      "Switch the {DeviceName} on"
      "Turn on the {DeviceName}"
    GetDeviceStatus:
      "Is the {DeviceName} on[?]"
    TurnOffDevice:
      "Turn the {DeviceName} off"
      "Switch off the {DeviceName}"
      "Turn off the {DeviceName}"
    ```
* When you teach your model with each different type of utterance, the Azure AI Language service can learn how to categorize intents correctly based off format and punctuation.

### Use pre-built entity components
* create your own language models by defining all the intents and utterances it requires, but often you can use prebuilt components to detect common entities such as numbers, emails, URLs, or choices.
* [For a full list of prebuilt entities the Azure AI Language service can detect](https://learn.microsoft.com/en-us/azure/ai-services/language-service/conversational-language-understanding/prebuilt-component-reference).
* Using prebuilt components allows you to let the Azure AI Language service automatically detect the specified type of entity, and not have to train your model with examples of that entity.
* To add a prebuilt component, you can create an entity in your project, then select Add new prebuilt to that entity to detect certain entities.
* ![](https://learn.microsoft.com/en-us/training/wwl-data-ai/build-language-understanding-model/media/add-prebuilt-entity-small.png)
* You can have up to five prebuilt components per entity. 
* Using prebuilt model elements can significantly reduce the time it takes to develop a conversational language understanding solution.
